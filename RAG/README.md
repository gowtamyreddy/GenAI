# ğŸ§  RAG Pipeline using Hugging Face, ChromaDB, and Groq LLM

---

## Goal

The goal of this project is to build a **Retrieval-Augmented Generation (RAG)** pipeline that:

- Uses **external knowledge sources** (e.g., websites) as context  
- Retrieves **semantically relevant content** for user queries  
- Generates accurate, grounded answers using **Groq-hosted LLaMA3 models**

---

## How It Works

1. Scrape live content using `WebBaseLoader`  
2. Split long text into manageable chunks  
3. Embed chunks with `all-MiniLM-L6-v2` from Hugging Face  
4. Store embeddings in **ChromaDB** (local vector store)  
5. Retrieve top-matching chunks using vector similarity  
6. Pass retrieved context into Groq's LLaMA3 model  
7. Generate intelligent, concise answers with LangChain  

---

## Steps Overview

### ğŸ”§ Step 1: Install Dependencies

Install required Python packages using pip.

### ğŸ” Step 2: Set Your Groq API Key

Set the `GROQ_API_KEY` environment variable from your Groq Console.

### ğŸŒ Step 3: Load Web Content

Use `WebBaseLoader` to pull live data (e.g., from IBMâ€™s Machine Learning page).

### âœ‚ï¸ Step 4: Chunk the Text

Split documents into overlapping chunks for efficient retrieval.

### ğŸ§  Step 5: Generate Embeddings & Store in ChromaDB

Use `all-MiniLM-L6-v2` to convert text into embeddings and store them locally.

### ğŸ” Step 6: Retrieve Relevant Chunks

Use semantic similarity search to find the most relevant chunks for any query.

### ğŸ¤– Step 7: Build the RAG Pipeline

Connect retriever â†’ prompt â†’ Groq LLM using LangChainâ€™s modular components.

### ğŸ’¬ Step 8: Ask Questions

Run queries like:

- â€œWhat is Machine Learning?â€  
- â€œWhat are some use cases of ML?â€  
- â€œWhatâ€™s the difference between ML and DL?â€  

Groqâ€™s LLM will respond based on retrieved knowledge chunks with fast, accurate answers.
